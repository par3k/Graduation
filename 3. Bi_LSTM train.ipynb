{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Bi_LSTM train.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPpBJ3xGq+ivJRUEp06yjiM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JmBpV5JQgIx5","colab_type":"text"},"source":["# **Basically set the modules**"]},{"cell_type":"code","metadata":{"id":"UfJdTYDav6pq","colab_type":"code","outputId":"336369cf-b383-4124-f08b-0201f714cbef","executionInfo":{"status":"ok","timestamp":1587459647957,"user_tz":-540,"elapsed":28652,"user":{"displayName":"박회재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLNtW3cEP6ijcjRJLZBANmZYUyTrHXcKJWEGtNDA=s64","userId":"17044052781042873598"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["# Moundt the G Drive\n","\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YiJfHJLbuAzf","colab_type":"code","outputId":"9e34a4a5-b641-4b1f-aba2-3f1b249c286f","executionInfo":{"status":"ok","timestamp":1587459661122,"user_tz":-540,"elapsed":9363,"user":{"displayName":"박회재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLNtW3cEP6ijcjRJLZBANmZYUyTrHXcKJWEGtNDA=s64","userId":"17044052781042873598"}},"colab":{"base_uri":"https://localhost:8080/","height":600}},"source":["!pip install konlpy"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting konlpy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n","\u001b[K     |████████████████████████████████| 19.4MB 1.4MB/s \n","\u001b[?25hCollecting JPype1>=0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/3c/1dbe5d6943b5c68e8df17c8b3a05db4725eadb5c7b7de437506aa3030701/JPype1-0.7.2-cp36-cp36m-manylinux1_x86_64.whl (2.4MB)\n","\u001b[K     |████████████████████████████████| 2.4MB 24.8MB/s \n","\u001b[?25hCollecting tweepy>=3.7.0\n","  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n","Collecting beautifulsoup4==4.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n","\u001b[K     |████████████████████████████████| 92kB 5.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n","Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.21.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2020.4.5.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n","Installing collected packages: JPype1, tweepy, colorama, beautifulsoup4, konlpy\n","  Found existing installation: tweepy 3.6.0\n","    Uninstalling tweepy-3.6.0:\n","      Successfully uninstalled tweepy-3.6.0\n","  Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-0.7.2 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OIiD0y_nt2t9","colab_type":"code","outputId":"d3132c2f-e320-4851-8cb6-3242bb59cd4b","executionInfo":{"status":"ok","timestamp":1587459672277,"user_tz":-540,"elapsed":7062,"user":{"displayName":"박회재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLNtW3cEP6ijcjRJLZBANmZYUyTrHXcKJWEGtNDA=s64","userId":"17044052781042873598"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%tensorflow_version 1.x\n","import time\n","import os\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","from konlpy.tag import Okt\n","import numpy as np\n","import gensim"],"execution_count":3,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E7wqslVh-E9L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d7af272e-653f-498e-8f69-d92f9a05ee80","executionInfo":{"status":"ok","timestamp":1587459679607,"user_tz":-540,"elapsed":723,"user":{"displayName":"박회재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLNtW3cEP6ijcjRJLZBANmZYUyTrHXcKJWEGtNDA=s64","userId":"17044052781042873598"}}},"source":["tf.__version__"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.15.2'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"R41Bf2Qmpzp-","colab_type":"text"},"source":["# **Define the Word2Vec, Bi_LSTM functions**"]},{"cell_type":"code","metadata":{"id":"Z58WktgGt8e0","colab_type":"code","colab":{}},"source":["class Word2Vec():\n","    \n","    def __init__(self):\n","        None\n","\n","    def tokenize(self, doc):\n","        pos_tagger = Okt()\n","        return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]\n","    \n","    def read_data(self, filename):\n","        with open(filename, 'r',encoding='utf-8') as f:\n","            data = [line.split('\\t') for line in f.read().splitlines()]\n","            data = data[1:]\n","        return data  \n","    \n","    def Word2vec_model(self, model_name):\n","        \n","        model = gensim.models.word2vec.Word2Vec.load(model_name)\n","        return model\n","    \n","    def Convert2Vec(self, model_name, doc):  # Convert corpus into vectors\n","        word_vec = []\n","        model = gensim.models.word2vec.Word2Vec.load(model_name)\n","        for sent in doc:\n","            sub = []\n","            for word in sent:\n","                if(word in model.wv.vocab):\n","                    sub.append(model.wv[word])\n","                else:\n","                    sub.append(np.random.uniform(-0.25,0.25,300)) # used for Out Of Vacabulary words\n","            word_vec.append(sub)\n","        \n","        return np.array(word_vec)\n","    \n","    def Zero_padding(self, train_batch_X, Batch_size, Maxseq_length, Vector_size):\n","        \n","        zero_pad = np.zeros((Batch_size, Maxseq_length, Vector_size))\n","        for i in range(Batch_size):\n","            zero_pad[i,:np.shape(train_batch_X[i])[0],:np.shape(train_batch_X[i])[1]] = train_batch_X[i]\n","            \n","        return zero_pad\n","    \n","    def One_hot(self, data):\n","       \n","        index_dict = {value:index for index,value in enumerate(set(data))}\n","        result = []\n","        \n","        for value in data:\n","            \n","            one_hot = np.zeros(len(index_dict))\n","            index = index_dict[value]\n","            one_hot[index] = 1\n","            result.append(one_hot)\n","        \n","        return np.array(result)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X-KHBQueuFCF","colab_type":"code","colab":{}},"source":["class Bi_LSTM():\n","    \n","    def __init__(self, lstm_units, num_class, keep_prob):\n","        \n","        self.lstm_units = lstm_units\n","        \n","        # Define forward LSTM layer\n","        with tf.variable_scope('forward', reuse = tf.AUTO_REUSE):\n","            \n","            self.lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_units, forget_bias=1.0, state_is_tuple=True)\n","            self.lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(self.lstm_fw_cell, output_keep_prob = keep_prob) # 데이터를 droupout 한다\n","            #드롭아웃은 합성곱 신경망의 오버피팅을 방지하는 가장 유명하면서도 흥미로운 방법이다.\n","            #드롭아웃 레이어는 뉴런의 일부를 확률적으로 ‘비활성화’한다.\n","            #이는 뉴런의 상호 적응을 방지하고 피쳐를 개별적으로 학습하도록 강제하여\n","            #사람이 그림을 맞출때 일부를 손으로 가린채 특징을 학습하여 맞추도록 하는 방식과 유사하게 동작한다.\n","\n","        # Define backward LSTM layer    \n","        with tf.variable_scope('backward', reuse = tf.AUTO_REUSE):\n","            \n","            self.lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_units, forget_bias=1.0, state_is_tuple=True)\n","            self.lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(self.lstm_bw_cell, output_keep_prob = keep_prob)\n","        \n","        # Define weight variables\n","        with tf.variable_scope('Weights', reuse = tf.AUTO_REUSE):\n","           \n","            self.W = tf.get_variable(name=\"W\", shape=[2 * lstm_units, num_class],\n","                                dtype=tf.float32, initializer = tf.contrib.layers.xavier_initializer())\n","            self.b = tf.get_variable(name=\"b\", shape=[num_class], dtype=tf.float32,\n","                                initializer=tf.zeros_initializer())\n","            \n","            \n","    def logits(self, X, W, b, seq_len):\n","        \n","        # Implement bidirectional_lstm using the function bidirectional_dynamic_rnn.\n","        (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(self.lstm_fw_cell, self.lstm_bw_cell,\n","                                                                         dtype=tf.float32, inputs = X, sequence_length = seq_len)\n","        # Concat fw, bw final states\n","        # Extract the last state value\n","        outputs = tf.concat([states[0][1], states[1][1]], axis=1)\n","\n","        # set the Linear Regression\n","        pred = tf.matmul(outputs, W) + b        \n","        return pred\n","        \n","    def model_build(self, logits, labels, learning_rate = 0.001):\n","        \n","        with tf.variable_scope(\"loss\"):\n","            \n","            # 손실함수로 크로스 엔트로피 오차를 사용한다\n","            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits , labels = labels)) # Softmax loss\n","            # 최적화는 손실함수를 이용해서 모델을 학습하는 방법으로\n","            # 손실함수의 결과값을 최소화하는 모델 인자를 찾는다\n","            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) # Adam Optimizer\n","            \n","        return loss, optimizer\n","    \n","    def graph_build(self):\n","        \n","        self.loss = tf.placeholder(tf.float32)\n","        self.acc = tf.placeholder(tf.float32)\n","        \n","        tf.summary.scalar('Loss', self.loss)\n","        tf.summary.scalar('Accuracy', self.acc)\n","        merged = tf.summary.merge_all()\n","        return merged"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VMVG7OS5n2n-","colab_type":"text"},"source":["손실함수 정의\n","\n","손실 함수란 신경망이 학습할 때, 정확도에 가까워질 수 있도록 최적화해주는 지표.\n","\n","\n","\n","머신러닝 모델의 출력값과 사용자가 원하는 출력값의 차이,\n","\n","즉 오차가 최소화되도록 하는 가중치와 편향을 찾는 것이 바로 학습이다.\n","\n","\n","\n","![대체 텍스트](https://www.slipp.net/wiki/download/attachments/30771160/image2018-11-7_19-42-48.png?version=1&modificationDate=1541587368000&api=v2)\n"]},{"cell_type":"markdown","metadata":{"id":"f4ge_XUmX_eg","colab_type":"text"},"source":["# **Train the Bi_LSTM model**"]},{"cell_type":"code","metadata":{"id":"mhwpfj2uuN1J","colab_type":"code","colab":{}},"source":["config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","\n","W2V = Word2Vec()\n","train_data = W2V.read_data('/content/gdrive/My Drive/Colab Notebooks/GraduationProject/Word2Vec/Movie_rating_data/ratings_train.txt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kB3DoLBvuOrx","colab_type":"code","outputId":"3504ae90-b8ca-47d2-8f45-20976e7ac348","executionInfo":{"status":"ok","timestamp":1587461459664,"user_tz":-540,"elapsed":1765857,"user":{"displayName":"박회재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLNtW3cEP6ijcjRJLZBANmZYUyTrHXcKJWEGtNDA=s64","userId":"17044052781042873598"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["## tokenize the data we have : about 30 mins\n","\n","print(\"Tokenize Start!\\nCould take minutes...\")\n","train_tokens = [[W2V.tokenize(row[1]),int(row[2])] for row in train_data if W2V.tokenize(row[1]) != []]\n","train_tokens = np.array(train_tokens)\n","print(\"Tokenize Done!\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Tokenize Start!\n","Could take minutes...\n","Tokenize Done!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F1iBqeoEujII","colab_type":"code","colab":{}},"source":["train_X = train_tokens[:,0]\n","train_Y = train_tokens[:,1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ccu2xLBxumrr","colab_type":"code","outputId":"15a35acf-a23a-464b-c2e7-cd8344854aae","executionInfo":{"status":"ok","timestamp":1587461825908,"user_tz":-540,"elapsed":9219,"user":{"displayName":"박회재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLNtW3cEP6ijcjRJLZBANmZYUyTrHXcKJWEGtNDA=s64","userId":"17044052781042873598"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["train_Y_ = W2V.One_hot(train_Y)  ## Convert to One-hot\n","train_X_ = W2V.Convert2Vec(\"/content/gdrive/My Drive/Colab Notebooks/GraduationProject/Word2Vec/Word2vec.model\",train_X)  ## import word2vec model where you have trained before"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ChUjXZewuszg","colab_type":"code","colab":{}},"source":["# set the Hyper parameter\n","Batch_size = 32\n","Total_size = len(train_X)\n","Vector_size = 300 # 임베딩 사이즈\n","train_seq_length = [len(x) for x in train_X]\n","Maxseq_length = max(train_seq_length) # 여기서는 95\n","learning_rate = 0.001 # 학습률\n","lstm_units = 128 # 128차원\n","num_class = 2 # negative and positive\n","# num_class = 12 # number of the news categories\n","training_epochs = 13\n","keep_prob = 0.75"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5223FQqouu2x","colab_type":"code","colab":{}},"source":["# 데이터가 들어가기때문에 데이터 갯수가 none으로 지정되고 maxseq_length, vector_size로 이루어진 input shape이다 \n","X = tf.placeholder(tf.float32, shape = [None, Maxseq_length, Vector_size], name = 'X')\n","# 데이터 갯수와 데이터가 긍정인지 부정인지를 나타내는 클래스 가 들어간 output shape이다\n","Y = tf.placeholder(tf.float32, shape = [None, num_class], name = 'Y')\n","seq_len = tf.placeholder(tf.int32, shape = [None])\n","keep_prob = tf.placeholder(tf.float32, shape = None) # dropout 할때 keep할 비율"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PsuvyRWuwlI","colab_type":"code","outputId":"456cbefe-04a1-4091-f92a-0224a1a66f0d","executionInfo":{"status":"ok","timestamp":1587461831563,"user_tz":-540,"elapsed":766,"user":{"displayName":"박회재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLNtW3cEP6ijcjRJLZBANmZYUyTrHXcKJWEGtNDA=s64","userId":"17044052781042873598"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["# Bi_LSTM을 불러온다\n","BiLSTM = Bi_LSTM(lstm_units, num_class, keep_prob)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-6-cfa074d9452c>:10: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i_jEs_n8uy7g","colab_type":"code","outputId":"fca2e118-39f4-415c-db9a-b6a4aad00e7c","executionInfo":{"status":"ok","timestamp":1587461835583,"user_tz":-540,"elapsed":2264,"user":{"displayName":"박회재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLNtW3cEP6ijcjRJLZBANmZYUyTrHXcKJWEGtNDA=s64","userId":"17044052781042873598"}},"colab":{"base_uri":"https://localhost:8080/","height":301}},"source":["# 로짓, 로스, 옵티마이저를 정의해준다\n","with tf.variable_scope(\"loss\", reuse = tf.AUTO_REUSE):\n","    logits = BiLSTM.logits(X, BiLSTM.W, BiLSTM.b, seq_len)\n","    loss, optimizer = BiLSTM.model_build(logits, Y, learning_rate)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-6-cfa074d9452c>:36: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DS9bV0cku1Yc","colab_type":"code","colab":{}},"source":["prediction = tf.nn.softmax(logits)\n","correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","\n","init = tf.global_variables_initializer()\n","\n","total_batch = int(len(train_X) / Batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYvlB3SEu-3Y","colab_type":"code","colab":{}},"source":["saver = tf.train.Saver()\n","\n","train_acc = []\n","train_loss = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vDhQlYNL8bfR","colab_type":"code","outputId":"4f0b279f-82d7-4d8f-b619-c6adb84030ed","executionInfo":{"status":"ok","timestamp":1587485100755,"user_tz":-540,"elapsed":23250082,"user":{"displayName":"박회재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLNtW3cEP6ijcjRJLZBANmZYUyTrHXcKJWEGtNDA=s64","userId":"17044052781042873598"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["sess = tf.Session(config = config)\n","sess.run(init)\n","train_writer = tf.summary.FileWriter('/content/gdrive/My Drive/Colab Notebooks/GraduationProject/Bi_LSTM', sess.graph)\n","merged = BiLSTM.graph_build()\n","\n","# 1 epoch about 40 mins\n","for epoch in range(training_epochs):\n","\n","    avg_acc, avg_loss = 0. , 0. # avg_acc, avg_loss initial value set\n","    mask = np.random.permutation(len(train_X_))\n","    train_X_ = train_X_[mask]\n","    train_Y_ = train_Y_[mask]\n","\n","\n","    for step in range(total_batch):\n","\n","        train_batch_X = train_X_[step*Batch_size : step*Batch_size+Batch_size]\n","        train_batch_Y = train_Y_[step*Batch_size : step*Batch_size+Batch_size]\n","        batch_seq_length = train_seq_length[step*Batch_size : step*Batch_size+Batch_size]\n","        \n","        # 각 배치마다 제로 패딩을 해준다\n","        train_batch_X = W2V.Zero_padding(train_batch_X, Batch_size, Maxseq_length, Vector_size)\n","        \n","        # 패딩을 해준 후에 옵티마이즈를 해준다\n","        sess.run(optimizer, feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n","\n","        # Compute average loss\n","        loss_ = sess.run(loss, feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length, keep_prob : 0.75})\n","        avg_loss += loss_ / total_batch\n","        \n","        acc = sess.run(accuracy , feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length, keep_prob : 0.75})\n","        avg_acc += acc / total_batch\n","        if step % 100 == 0:\n","          print(\"epoch : {:02d} step : {:04d} loss = {:.6f} accuracy= {:.6f}\".format(epoch+1, step+1, loss_, acc))\n","\n","save_path = saver.save(sess,'/content/gdrive/My Drive/Colab Notebooks/GraduationProject/Bi_LSTM/BiLSTM_model_Epoch_13.ckpt')\n","print ('Save Complete')\n","print ('save_path : ',save_path)\n","\n","sess.close()"],"execution_count":17,"outputs":[{"output_type":"stream","text":["epoch : 01 step : 0001 loss = 0.597761 accuracy= 0.656250\n","epoch : 01 step : 0101 loss = 0.559672 accuracy= 0.750000\n","epoch : 01 step : 0201 loss = 0.458909 accuracy= 0.750000\n","epoch : 01 step : 0301 loss = 0.522604 accuracy= 0.687500\n","epoch : 01 step : 0401 loss = 0.446483 accuracy= 0.875000\n","epoch : 01 step : 0501 loss = 0.368069 accuracy= 0.843750\n","epoch : 01 step : 0601 loss = 0.417968 accuracy= 0.750000\n","epoch : 01 step : 0701 loss = 0.409353 accuracy= 0.812500\n","epoch : 01 step : 0801 loss = 0.480773 accuracy= 0.750000\n","epoch : 01 step : 0901 loss = 0.588139 accuracy= 0.718750\n","epoch : 01 step : 1001 loss = 0.522390 accuracy= 0.812500\n","epoch : 01 step : 1101 loss = 0.461380 accuracy= 0.718750\n","epoch : 01 step : 1201 loss = 0.538001 accuracy= 0.750000\n","epoch : 01 step : 1301 loss = 0.472524 accuracy= 0.781250\n","epoch : 01 step : 1401 loss = 0.495677 accuracy= 0.781250\n","epoch : 01 step : 1501 loss = 0.333201 accuracy= 0.843750\n","epoch : 01 step : 1601 loss = 0.482548 accuracy= 0.812500\n","epoch : 01 step : 1701 loss = 0.404819 accuracy= 0.781250\n","epoch : 01 step : 1801 loss = 0.333028 accuracy= 0.875000\n","epoch : 01 step : 1901 loss = 0.391949 accuracy= 0.812500\n","epoch : 01 step : 2001 loss = 0.347650 accuracy= 0.843750\n","epoch : 01 step : 2101 loss = 0.400108 accuracy= 0.812500\n","epoch : 01 step : 2201 loss = 0.267810 accuracy= 0.875000\n","epoch : 01 step : 2301 loss = 0.442115 accuracy= 0.750000\n","epoch : 01 step : 2401 loss = 0.536670 accuracy= 0.718750\n","epoch : 01 step : 2501 loss = 0.374099 accuracy= 0.843750\n","epoch : 01 step : 2601 loss = 0.562137 accuracy= 0.750000\n","epoch : 01 step : 2701 loss = 0.487183 accuracy= 0.687500\n","epoch : 01 step : 2801 loss = 0.347892 accuracy= 0.875000\n","epoch : 01 step : 2901 loss = 0.405099 accuracy= 0.875000\n","epoch : 01 step : 3001 loss = 0.441919 accuracy= 0.843750\n","epoch : 01 step : 3101 loss = 0.442329 accuracy= 0.687500\n","epoch : 01 step : 3201 loss = 0.391831 accuracy= 0.843750\n","epoch : 01 step : 3301 loss = 0.386640 accuracy= 0.750000\n","epoch : 01 step : 3401 loss = 0.315167 accuracy= 0.875000\n","epoch : 01 step : 3501 loss = 0.596885 accuracy= 0.687500\n","epoch : 01 step : 3601 loss = 0.353865 accuracy= 0.875000\n","epoch : 01 step : 3701 loss = 0.398367 accuracy= 0.781250\n","epoch : 01 step : 3801 loss = 0.306361 accuracy= 0.875000\n","epoch : 01 step : 3901 loss = 0.511550 accuracy= 0.718750\n","epoch : 01 step : 4001 loss = 0.449323 accuracy= 0.781250\n","epoch : 01 step : 4101 loss = 0.316947 accuracy= 0.843750\n","epoch : 01 step : 4201 loss = 0.588992 accuracy= 0.812500\n","epoch : 01 step : 4301 loss = 0.365075 accuracy= 0.843750\n","epoch : 01 step : 4401 loss = 0.411505 accuracy= 0.781250\n","epoch : 01 step : 4501 loss = 0.473391 accuracy= 0.812500\n","epoch : 01 step : 4601 loss = 0.368509 accuracy= 0.843750\n","epoch : 02 step : 0001 loss = 0.385072 accuracy= 0.843750\n","epoch : 02 step : 0101 loss = 0.518233 accuracy= 0.687500\n","epoch : 02 step : 0201 loss = 0.474698 accuracy= 0.718750\n","epoch : 02 step : 0301 loss = 0.327162 accuracy= 0.812500\n","epoch : 02 step : 0401 loss = 0.359846 accuracy= 0.843750\n","epoch : 02 step : 0501 loss = 0.295245 accuracy= 0.875000\n","epoch : 02 step : 0601 loss = 0.392273 accuracy= 0.812500\n","epoch : 02 step : 0701 loss = 0.261225 accuracy= 0.906250\n","epoch : 02 step : 0801 loss = 0.356686 accuracy= 0.843750\n","epoch : 02 step : 0901 loss = 0.362251 accuracy= 0.875000\n","epoch : 02 step : 1001 loss = 0.434177 accuracy= 0.843750\n","epoch : 02 step : 1101 loss = 0.527256 accuracy= 0.718750\n","epoch : 02 step : 1201 loss = 0.551191 accuracy= 0.750000\n","epoch : 02 step : 1301 loss = 0.239650 accuracy= 0.937500\n","epoch : 02 step : 1401 loss = 0.442225 accuracy= 0.812500\n","epoch : 02 step : 1501 loss = 0.444581 accuracy= 0.718750\n","epoch : 02 step : 1601 loss = 0.363552 accuracy= 0.812500\n","epoch : 02 step : 1701 loss = 0.407474 accuracy= 0.843750\n","epoch : 02 step : 1801 loss = 0.394491 accuracy= 0.781250\n","epoch : 02 step : 1901 loss = 0.385686 accuracy= 0.718750\n","epoch : 02 step : 2001 loss = 0.340888 accuracy= 0.875000\n","epoch : 02 step : 2101 loss = 0.347890 accuracy= 0.875000\n","epoch : 02 step : 2201 loss = 0.658528 accuracy= 0.750000\n","epoch : 02 step : 2301 loss = 0.341582 accuracy= 0.843750\n","epoch : 02 step : 2401 loss = 0.387874 accuracy= 0.843750\n","epoch : 02 step : 2501 loss = 0.465835 accuracy= 0.718750\n","epoch : 02 step : 2601 loss = 0.333143 accuracy= 0.843750\n","epoch : 02 step : 2701 loss = 0.648308 accuracy= 0.718750\n","epoch : 02 step : 2801 loss = 0.215434 accuracy= 0.937500\n","epoch : 02 step : 2901 loss = 0.407066 accuracy= 0.812500\n","epoch : 02 step : 3001 loss = 0.450157 accuracy= 0.781250\n","epoch : 02 step : 3101 loss = 0.267781 accuracy= 0.843750\n","epoch : 02 step : 3201 loss = 0.433692 accuracy= 0.781250\n","epoch : 02 step : 3301 loss = 0.482009 accuracy= 0.750000\n","epoch : 02 step : 3401 loss = 0.260209 accuracy= 0.906250\n","epoch : 02 step : 3501 loss = 0.423303 accuracy= 0.781250\n","epoch : 02 step : 3601 loss = 0.288690 accuracy= 0.875000\n","epoch : 02 step : 3701 loss = 0.415393 accuracy= 0.812500\n","epoch : 02 step : 3801 loss = 0.432491 accuracy= 0.781250\n","epoch : 02 step : 3901 loss = 0.294831 accuracy= 0.875000\n","epoch : 02 step : 4001 loss = 0.274426 accuracy= 0.906250\n","epoch : 02 step : 4101 loss = 0.351658 accuracy= 0.843750\n","epoch : 02 step : 4201 loss = 0.603935 accuracy= 0.656250\n","epoch : 02 step : 4301 loss = 0.166443 accuracy= 0.937500\n","epoch : 02 step : 4401 loss = 0.432501 accuracy= 0.781250\n","epoch : 02 step : 4501 loss = 0.314725 accuracy= 0.843750\n","epoch : 02 step : 4601 loss = 0.385944 accuracy= 0.812500\n","epoch : 03 step : 0001 loss = 0.327961 accuracy= 0.812500\n","epoch : 03 step : 0101 loss = 0.428366 accuracy= 0.781250\n","epoch : 03 step : 0201 loss = 0.372261 accuracy= 0.843750\n","epoch : 03 step : 0301 loss = 0.358575 accuracy= 0.812500\n","epoch : 03 step : 0401 loss = 0.349467 accuracy= 0.906250\n","epoch : 03 step : 0501 loss = 0.287905 accuracy= 0.843750\n","epoch : 03 step : 0601 loss = 0.264512 accuracy= 0.875000\n","epoch : 03 step : 0701 loss = 0.355629 accuracy= 0.843750\n","epoch : 03 step : 0801 loss = 0.479245 accuracy= 0.750000\n","epoch : 03 step : 0901 loss = 0.317395 accuracy= 0.875000\n","epoch : 03 step : 1001 loss = 0.411074 accuracy= 0.750000\n","epoch : 03 step : 1101 loss = 0.230307 accuracy= 0.875000\n","epoch : 03 step : 1201 loss = 0.332147 accuracy= 0.875000\n","epoch : 03 step : 1301 loss = 0.276148 accuracy= 0.875000\n","epoch : 03 step : 1401 loss = 0.467522 accuracy= 0.843750\n","epoch : 03 step : 1501 loss = 0.362148 accuracy= 0.843750\n","epoch : 03 step : 1601 loss = 0.252117 accuracy= 0.875000\n","epoch : 03 step : 1701 loss = 0.253162 accuracy= 0.906250\n","epoch : 03 step : 1801 loss = 0.294589 accuracy= 0.875000\n","epoch : 03 step : 1901 loss = 0.259812 accuracy= 0.906250\n","epoch : 03 step : 2001 loss = 0.349096 accuracy= 0.750000\n","epoch : 03 step : 2101 loss = 0.316751 accuracy= 0.875000\n","epoch : 03 step : 2201 loss = 0.373148 accuracy= 0.750000\n","epoch : 03 step : 2301 loss = 0.265906 accuracy= 0.843750\n","epoch : 03 step : 2401 loss = 0.424679 accuracy= 0.843750\n","epoch : 03 step : 2501 loss = 0.424264 accuracy= 0.750000\n","epoch : 03 step : 2601 loss = 0.384775 accuracy= 0.843750\n","epoch : 03 step : 2701 loss = 0.555934 accuracy= 0.656250\n","epoch : 03 step : 2801 loss = 0.099130 accuracy= 1.000000\n","epoch : 03 step : 2901 loss = 0.179280 accuracy= 0.968750\n","epoch : 03 step : 3001 loss = 0.336429 accuracy= 0.843750\n","epoch : 03 step : 3101 loss = 0.289592 accuracy= 0.812500\n","epoch : 03 step : 3201 loss = 0.393032 accuracy= 0.843750\n","epoch : 03 step : 3301 loss = 0.503102 accuracy= 0.718750\n","epoch : 03 step : 3401 loss = 0.374330 accuracy= 0.812500\n","epoch : 03 step : 3501 loss = 0.335785 accuracy= 0.812500\n","epoch : 03 step : 3601 loss = 0.352500 accuracy= 0.781250\n","epoch : 03 step : 3701 loss = 0.255224 accuracy= 0.875000\n","epoch : 03 step : 3801 loss = 0.351361 accuracy= 0.812500\n","epoch : 03 step : 3901 loss = 0.472250 accuracy= 0.687500\n","epoch : 03 step : 4001 loss = 0.486726 accuracy= 0.750000\n","epoch : 03 step : 4101 loss = 0.261034 accuracy= 0.875000\n","epoch : 03 step : 4201 loss = 0.459060 accuracy= 0.843750\n","epoch : 03 step : 4301 loss = 0.462893 accuracy= 0.812500\n","epoch : 03 step : 4401 loss = 0.310005 accuracy= 0.812500\n","epoch : 03 step : 4501 loss = 0.395021 accuracy= 0.843750\n","epoch : 03 step : 4601 loss = 0.239333 accuracy= 0.968750\n","epoch : 04 step : 0001 loss = 0.263202 accuracy= 0.875000\n","epoch : 04 step : 0101 loss = 0.286592 accuracy= 0.906250\n","epoch : 04 step : 0201 loss = 0.278487 accuracy= 0.875000\n","epoch : 04 step : 0301 loss = 0.462524 accuracy= 0.781250\n","epoch : 04 step : 0401 loss = 0.279636 accuracy= 0.906250\n","epoch : 04 step : 0501 loss = 0.353249 accuracy= 0.875000\n","epoch : 04 step : 0601 loss = 0.317675 accuracy= 0.812500\n","epoch : 04 step : 0701 loss = 0.320200 accuracy= 0.875000\n","epoch : 04 step : 0801 loss = 0.371781 accuracy= 0.843750\n","epoch : 04 step : 0901 loss = 0.231334 accuracy= 0.937500\n","epoch : 04 step : 1001 loss = 0.366136 accuracy= 0.781250\n","epoch : 04 step : 1101 loss = 0.430827 accuracy= 0.781250\n","epoch : 04 step : 1201 loss = 0.357725 accuracy= 0.843750\n","epoch : 04 step : 1301 loss = 0.411012 accuracy= 0.843750\n","epoch : 04 step : 1401 loss = 0.228567 accuracy= 0.937500\n","epoch : 04 step : 1501 loss = 0.390331 accuracy= 0.906250\n","epoch : 04 step : 1601 loss = 0.260016 accuracy= 0.875000\n","epoch : 04 step : 1701 loss = 0.341696 accuracy= 0.843750\n","epoch : 04 step : 1801 loss = 0.402258 accuracy= 0.781250\n","epoch : 04 step : 1901 loss = 0.290762 accuracy= 0.937500\n","epoch : 04 step : 2001 loss = 0.455602 accuracy= 0.812500\n","epoch : 04 step : 2101 loss = 0.352342 accuracy= 0.812500\n","epoch : 04 step : 2201 loss = 0.409679 accuracy= 0.812500\n","epoch : 04 step : 2301 loss = 0.318597 accuracy= 0.875000\n","epoch : 04 step : 2401 loss = 0.384951 accuracy= 0.750000\n","epoch : 04 step : 2501 loss = 0.302332 accuracy= 0.906250\n","epoch : 04 step : 2601 loss = 0.335809 accuracy= 0.875000\n","epoch : 04 step : 2701 loss = 0.338975 accuracy= 0.843750\n","epoch : 04 step : 2801 loss = 0.213744 accuracy= 0.906250\n","epoch : 04 step : 2901 loss = 0.317740 accuracy= 0.875000\n","epoch : 04 step : 3001 loss = 0.367001 accuracy= 0.781250\n","epoch : 04 step : 3101 loss = 0.334006 accuracy= 0.781250\n","epoch : 04 step : 3201 loss = 0.403880 accuracy= 0.781250\n","epoch : 04 step : 3301 loss = 0.290893 accuracy= 0.843750\n","epoch : 04 step : 3401 loss = 0.397048 accuracy= 0.781250\n","epoch : 04 step : 3501 loss = 0.324342 accuracy= 0.906250\n","epoch : 04 step : 3601 loss = 0.208077 accuracy= 0.937500\n","epoch : 04 step : 3701 loss = 0.231085 accuracy= 0.906250\n","epoch : 04 step : 3801 loss = 0.256404 accuracy= 0.937500\n","epoch : 04 step : 3901 loss = 0.476129 accuracy= 0.781250\n","epoch : 04 step : 4001 loss = 0.387769 accuracy= 0.750000\n","epoch : 04 step : 4101 loss = 0.320789 accuracy= 0.843750\n","epoch : 04 step : 4201 loss = 0.367860 accuracy= 0.781250\n","epoch : 04 step : 4301 loss = 0.415957 accuracy= 0.781250\n","epoch : 04 step : 4401 loss = 0.171774 accuracy= 0.968750\n","epoch : 04 step : 4501 loss = 0.393836 accuracy= 0.937500\n","epoch : 04 step : 4601 loss = 0.214244 accuracy= 0.937500\n","epoch : 05 step : 0001 loss = 0.262901 accuracy= 0.968750\n","epoch : 05 step : 0101 loss = 0.259787 accuracy= 0.906250\n","epoch : 05 step : 0201 loss = 0.300070 accuracy= 0.937500\n","epoch : 05 step : 0301 loss = 0.427127 accuracy= 0.875000\n","epoch : 05 step : 0401 loss = 0.232730 accuracy= 0.875000\n","epoch : 05 step : 0501 loss = 0.386452 accuracy= 0.812500\n","epoch : 05 step : 0601 loss = 0.214959 accuracy= 0.937500\n","epoch : 05 step : 0701 loss = 0.408195 accuracy= 0.812500\n","epoch : 05 step : 0801 loss = 0.308564 accuracy= 0.812500\n","epoch : 05 step : 0901 loss = 0.488022 accuracy= 0.812500\n","epoch : 05 step : 1001 loss = 0.450608 accuracy= 0.781250\n","epoch : 05 step : 1101 loss = 0.217959 accuracy= 0.968750\n","epoch : 05 step : 1201 loss = 0.561654 accuracy= 0.781250\n","epoch : 05 step : 1301 loss = 0.281338 accuracy= 0.906250\n","epoch : 05 step : 1401 loss = 0.392972 accuracy= 0.812500\n","epoch : 05 step : 1501 loss = 0.302790 accuracy= 0.812500\n","epoch : 05 step : 1601 loss = 0.297746 accuracy= 0.906250\n","epoch : 05 step : 1701 loss = 0.175039 accuracy= 0.968750\n","epoch : 05 step : 1801 loss = 0.294355 accuracy= 0.875000\n","epoch : 05 step : 1901 loss = 0.291805 accuracy= 0.812500\n","epoch : 05 step : 2001 loss = 0.293617 accuracy= 0.875000\n","epoch : 05 step : 2101 loss = 0.234641 accuracy= 0.875000\n","epoch : 05 step : 2201 loss = 0.218310 accuracy= 0.937500\n","epoch : 05 step : 2301 loss = 0.407412 accuracy= 0.812500\n","epoch : 05 step : 2401 loss = 0.378824 accuracy= 0.843750\n","epoch : 05 step : 2501 loss = 0.235089 accuracy= 0.843750\n","epoch : 05 step : 2601 loss = 0.470748 accuracy= 0.718750\n","epoch : 05 step : 2701 loss = 0.322383 accuracy= 0.843750\n","epoch : 05 step : 2801 loss = 0.256593 accuracy= 0.906250\n","epoch : 05 step : 2901 loss = 0.376778 accuracy= 0.843750\n","epoch : 05 step : 3001 loss = 0.200482 accuracy= 0.937500\n","epoch : 05 step : 3101 loss = 0.357112 accuracy= 0.843750\n","epoch : 05 step : 3201 loss = 0.375399 accuracy= 0.843750\n","epoch : 05 step : 3301 loss = 0.233770 accuracy= 0.875000\n","epoch : 05 step : 3401 loss = 0.236171 accuracy= 0.875000\n","epoch : 05 step : 3501 loss = 0.290697 accuracy= 0.843750\n","epoch : 05 step : 3601 loss = 0.146850 accuracy= 0.968750\n","epoch : 05 step : 3701 loss = 0.558680 accuracy= 0.750000\n","epoch : 05 step : 3801 loss = 0.246931 accuracy= 0.906250\n","epoch : 05 step : 3901 loss = 0.453450 accuracy= 0.750000\n","epoch : 05 step : 4001 loss = 0.328903 accuracy= 0.875000\n","epoch : 05 step : 4101 loss = 0.366596 accuracy= 0.812500\n","epoch : 05 step : 4201 loss = 0.298346 accuracy= 0.812500\n","epoch : 05 step : 4301 loss = 0.147063 accuracy= 0.968750\n","epoch : 05 step : 4401 loss = 0.173289 accuracy= 0.937500\n","epoch : 05 step : 4501 loss = 0.150166 accuracy= 1.000000\n","epoch : 05 step : 4601 loss = 0.320619 accuracy= 0.812500\n","epoch : 06 step : 0001 loss = 0.315440 accuracy= 0.843750\n","epoch : 06 step : 0101 loss = 0.140629 accuracy= 0.937500\n","epoch : 06 step : 0201 loss = 0.216480 accuracy= 0.875000\n","epoch : 06 step : 0301 loss = 0.238852 accuracy= 0.875000\n","epoch : 06 step : 0401 loss = 0.412329 accuracy= 0.781250\n","epoch : 06 step : 0501 loss = 0.333488 accuracy= 0.843750\n","epoch : 06 step : 0601 loss = 0.250777 accuracy= 0.875000\n","epoch : 06 step : 0701 loss = 0.285499 accuracy= 0.906250\n","epoch : 06 step : 0801 loss = 0.236249 accuracy= 0.843750\n","epoch : 06 step : 0901 loss = 0.335634 accuracy= 0.843750\n","epoch : 06 step : 1001 loss = 0.310657 accuracy= 0.843750\n","epoch : 06 step : 1101 loss = 0.217517 accuracy= 0.906250\n","epoch : 06 step : 1201 loss = 0.225623 accuracy= 0.906250\n","epoch : 06 step : 1301 loss = 0.365183 accuracy= 0.906250\n","epoch : 06 step : 1401 loss = 0.282490 accuracy= 0.937500\n","epoch : 06 step : 1501 loss = 0.287548 accuracy= 0.843750\n","epoch : 06 step : 1601 loss = 0.241850 accuracy= 0.875000\n","epoch : 06 step : 1701 loss = 0.141922 accuracy= 0.906250\n","epoch : 06 step : 1801 loss = 0.277014 accuracy= 0.812500\n","epoch : 06 step : 1901 loss = 0.518889 accuracy= 0.781250\n","epoch : 06 step : 2001 loss = 0.258458 accuracy= 0.906250\n","epoch : 06 step : 2101 loss = 0.525077 accuracy= 0.781250\n","epoch : 06 step : 2201 loss = 0.369211 accuracy= 0.875000\n","epoch : 06 step : 2301 loss = 0.309828 accuracy= 0.843750\n","epoch : 06 step : 2401 loss = 0.251452 accuracy= 0.875000\n","epoch : 06 step : 2501 loss = 0.237984 accuracy= 0.906250\n","epoch : 06 step : 2601 loss = 0.211872 accuracy= 0.906250\n","epoch : 06 step : 2701 loss = 0.109835 accuracy= 0.968750\n","epoch : 06 step : 2801 loss = 0.383688 accuracy= 0.843750\n","epoch : 06 step : 2901 loss = 0.522295 accuracy= 0.843750\n","epoch : 06 step : 3001 loss = 0.155620 accuracy= 0.937500\n","epoch : 06 step : 3101 loss = 0.266795 accuracy= 0.781250\n","epoch : 06 step : 3201 loss = 0.146576 accuracy= 1.000000\n","epoch : 06 step : 3301 loss = 0.246163 accuracy= 0.875000\n","epoch : 06 step : 3401 loss = 0.348099 accuracy= 0.812500\n","epoch : 06 step : 3501 loss = 0.242846 accuracy= 0.875000\n","epoch : 06 step : 3601 loss = 0.284795 accuracy= 0.875000\n","epoch : 06 step : 3701 loss = 0.200587 accuracy= 0.906250\n","epoch : 06 step : 3801 loss = 0.231423 accuracy= 0.906250\n","epoch : 06 step : 3901 loss = 0.288378 accuracy= 0.875000\n","epoch : 06 step : 4001 loss = 0.333774 accuracy= 0.812500\n","epoch : 06 step : 4101 loss = 0.520350 accuracy= 0.781250\n","epoch : 06 step : 4201 loss = 0.303952 accuracy= 0.875000\n","epoch : 06 step : 4301 loss = 0.258921 accuracy= 0.875000\n","epoch : 06 step : 4401 loss = 0.184510 accuracy= 0.968750\n","epoch : 06 step : 4501 loss = 0.336630 accuracy= 0.875000\n","epoch : 06 step : 4601 loss = 0.319713 accuracy= 0.843750\n","epoch : 07 step : 0001 loss = 0.159084 accuracy= 0.968750\n","epoch : 07 step : 0101 loss = 0.280088 accuracy= 0.875000\n","epoch : 07 step : 0201 loss = 0.425924 accuracy= 0.875000\n","epoch : 07 step : 0301 loss = 0.484883 accuracy= 0.781250\n","epoch : 07 step : 0401 loss = 0.306051 accuracy= 0.906250\n","epoch : 07 step : 0501 loss = 0.151447 accuracy= 1.000000\n","epoch : 07 step : 0601 loss = 0.334434 accuracy= 0.812500\n","epoch : 07 step : 0701 loss = 0.194390 accuracy= 0.875000\n","epoch : 07 step : 0801 loss = 0.260792 accuracy= 0.906250\n","epoch : 07 step : 0901 loss = 0.278409 accuracy= 0.937500\n","epoch : 07 step : 1001 loss = 0.283639 accuracy= 0.875000\n","epoch : 07 step : 1101 loss = 0.337844 accuracy= 0.781250\n","epoch : 07 step : 1201 loss = 0.204977 accuracy= 0.812500\n","epoch : 07 step : 1301 loss = 0.304075 accuracy= 0.875000\n","epoch : 07 step : 1401 loss = 0.199239 accuracy= 0.937500\n","epoch : 07 step : 1501 loss = 0.174898 accuracy= 0.906250\n","epoch : 07 step : 1601 loss = 0.193142 accuracy= 0.906250\n","epoch : 07 step : 1701 loss = 0.327477 accuracy= 0.875000\n","epoch : 07 step : 1801 loss = 0.281052 accuracy= 0.843750\n","epoch : 07 step : 1901 loss = 0.330764 accuracy= 0.812500\n","epoch : 07 step : 2001 loss = 0.371178 accuracy= 0.843750\n","epoch : 07 step : 2101 loss = 0.330334 accuracy= 0.843750\n","epoch : 07 step : 2201 loss = 0.302657 accuracy= 0.875000\n","epoch : 07 step : 2301 loss = 0.188594 accuracy= 0.968750\n","epoch : 07 step : 2401 loss = 0.194034 accuracy= 0.843750\n","epoch : 07 step : 2501 loss = 0.385050 accuracy= 0.843750\n","epoch : 07 step : 2601 loss = 0.224158 accuracy= 0.875000\n","epoch : 07 step : 2701 loss = 0.284076 accuracy= 0.906250\n","epoch : 07 step : 2801 loss = 0.316589 accuracy= 0.812500\n","epoch : 07 step : 2901 loss = 0.271275 accuracy= 0.906250\n","epoch : 07 step : 3001 loss = 0.274810 accuracy= 0.906250\n","epoch : 07 step : 3101 loss = 0.228928 accuracy= 0.906250\n","epoch : 07 step : 3201 loss = 0.326138 accuracy= 0.812500\n","epoch : 07 step : 3301 loss = 0.246296 accuracy= 0.906250\n","epoch : 07 step : 3401 loss = 0.121871 accuracy= 0.968750\n","epoch : 07 step : 3501 loss = 0.533244 accuracy= 0.781250\n","epoch : 07 step : 3601 loss = 0.206156 accuracy= 0.906250\n","epoch : 07 step : 3701 loss = 0.263793 accuracy= 0.843750\n","epoch : 07 step : 3801 loss = 0.255946 accuracy= 0.843750\n","epoch : 07 step : 3901 loss = 0.240343 accuracy= 0.937500\n","epoch : 07 step : 4001 loss = 0.279858 accuracy= 0.875000\n","epoch : 07 step : 4101 loss = 0.302092 accuracy= 0.843750\n","epoch : 07 step : 4201 loss = 0.258930 accuracy= 0.906250\n","epoch : 07 step : 4301 loss = 0.327019 accuracy= 0.843750\n","epoch : 07 step : 4401 loss = 0.315874 accuracy= 0.781250\n","epoch : 07 step : 4501 loss = 0.379251 accuracy= 0.875000\n","epoch : 07 step : 4601 loss = 0.289675 accuracy= 0.843750\n","epoch : 08 step : 0001 loss = 0.267356 accuracy= 0.937500\n","epoch : 08 step : 0101 loss = 0.188072 accuracy= 0.906250\n","epoch : 08 step : 0201 loss = 0.224103 accuracy= 0.906250\n","epoch : 08 step : 0301 loss = 0.278009 accuracy= 0.812500\n","epoch : 08 step : 0401 loss = 0.239885 accuracy= 0.843750\n","epoch : 08 step : 0501 loss = 0.242463 accuracy= 0.875000\n","epoch : 08 step : 0601 loss = 0.091601 accuracy= 1.000000\n","epoch : 08 step : 0701 loss = 0.107164 accuracy= 0.968750\n","epoch : 08 step : 0801 loss = 0.200798 accuracy= 0.906250\n","epoch : 08 step : 0901 loss = 0.192134 accuracy= 0.968750\n","epoch : 08 step : 1001 loss = 0.228324 accuracy= 0.937500\n","epoch : 08 step : 1101 loss = 0.159885 accuracy= 0.937500\n","epoch : 08 step : 1201 loss = 0.175253 accuracy= 0.968750\n","epoch : 08 step : 1301 loss = 0.227127 accuracy= 0.843750\n","epoch : 08 step : 1401 loss = 0.253013 accuracy= 0.843750\n","epoch : 08 step : 1501 loss = 0.103930 accuracy= 0.968750\n","epoch : 08 step : 1601 loss = 0.534355 accuracy= 0.843750\n","epoch : 08 step : 1701 loss = 0.172460 accuracy= 0.968750\n","epoch : 08 step : 1801 loss = 0.210729 accuracy= 0.875000\n","epoch : 08 step : 1901 loss = 0.334667 accuracy= 0.875000\n","epoch : 08 step : 2001 loss = 0.215659 accuracy= 0.875000\n","epoch : 08 step : 2101 loss = 0.369839 accuracy= 0.781250\n","epoch : 08 step : 2201 loss = 0.325233 accuracy= 0.812500\n","epoch : 08 step : 2301 loss = 0.104376 accuracy= 0.937500\n","epoch : 08 step : 2401 loss = 0.268263 accuracy= 0.875000\n","epoch : 08 step : 2501 loss = 0.372040 accuracy= 0.843750\n","epoch : 08 step : 2601 loss = 0.329192 accuracy= 0.843750\n","epoch : 08 step : 2701 loss = 0.249615 accuracy= 0.875000\n","epoch : 08 step : 2801 loss = 0.194771 accuracy= 0.906250\n","epoch : 08 step : 2901 loss = 0.266522 accuracy= 0.843750\n","epoch : 08 step : 3001 loss = 0.264220 accuracy= 0.906250\n","epoch : 08 step : 3101 loss = 0.372275 accuracy= 0.906250\n","epoch : 08 step : 3201 loss = 0.167111 accuracy= 0.906250\n","epoch : 08 step : 3301 loss = 0.289115 accuracy= 0.875000\n","epoch : 08 step : 3401 loss = 0.285892 accuracy= 0.875000\n","epoch : 08 step : 3501 loss = 0.235993 accuracy= 0.875000\n","epoch : 08 step : 3601 loss = 0.387552 accuracy= 0.781250\n","epoch : 08 step : 3701 loss = 0.207798 accuracy= 0.906250\n","epoch : 08 step : 3801 loss = 0.214626 accuracy= 0.906250\n","epoch : 08 step : 3901 loss = 0.155427 accuracy= 0.937500\n","epoch : 08 step : 4001 loss = 0.304088 accuracy= 0.906250\n","epoch : 08 step : 4101 loss = 0.252953 accuracy= 0.843750\n","epoch : 08 step : 4201 loss = 0.230589 accuracy= 0.937500\n","epoch : 08 step : 4301 loss = 0.105262 accuracy= 0.968750\n","epoch : 08 step : 4401 loss = 0.349326 accuracy= 0.812500\n","epoch : 08 step : 4501 loss = 0.190625 accuracy= 0.968750\n","epoch : 08 step : 4601 loss = 0.234260 accuracy= 0.875000\n","epoch : 09 step : 0001 loss = 0.097035 accuracy= 1.000000\n","epoch : 09 step : 0101 loss = 0.182448 accuracy= 0.937500\n","epoch : 09 step : 0201 loss = 0.171699 accuracy= 0.968750\n","epoch : 09 step : 0301 loss = 0.149971 accuracy= 1.000000\n","epoch : 09 step : 0401 loss = 0.341763 accuracy= 0.843750\n","epoch : 09 step : 0501 loss = 0.179064 accuracy= 0.937500\n","epoch : 09 step : 0601 loss = 0.367592 accuracy= 0.812500\n","epoch : 09 step : 0701 loss = 0.233653 accuracy= 0.843750\n","epoch : 09 step : 0801 loss = 0.392167 accuracy= 0.843750\n","epoch : 09 step : 0901 loss = 0.346480 accuracy= 0.843750\n","epoch : 09 step : 1001 loss = 0.254537 accuracy= 0.906250\n","epoch : 09 step : 1101 loss = 0.249329 accuracy= 0.875000\n","epoch : 09 step : 1201 loss = 0.193910 accuracy= 0.937500\n","epoch : 09 step : 1301 loss = 0.304597 accuracy= 0.906250\n","epoch : 09 step : 1401 loss = 0.187283 accuracy= 0.906250\n","epoch : 09 step : 1501 loss = 0.104663 accuracy= 0.968750\n","epoch : 09 step : 1601 loss = 0.213506 accuracy= 0.937500\n","epoch : 09 step : 1701 loss = 0.259853 accuracy= 0.906250\n","epoch : 09 step : 1801 loss = 0.211970 accuracy= 0.843750\n","epoch : 09 step : 1901 loss = 0.258324 accuracy= 0.843750\n","epoch : 09 step : 2001 loss = 0.159242 accuracy= 0.937500\n","epoch : 09 step : 2101 loss = 0.526994 accuracy= 0.812500\n","epoch : 09 step : 2201 loss = 0.151794 accuracy= 0.906250\n","epoch : 09 step : 2301 loss = 0.248408 accuracy= 0.875000\n","epoch : 09 step : 2401 loss = 0.224509 accuracy= 0.906250\n","epoch : 09 step : 2501 loss = 0.292021 accuracy= 0.875000\n","epoch : 09 step : 2601 loss = 0.231209 accuracy= 0.906250\n","epoch : 09 step : 2701 loss = 0.264828 accuracy= 0.843750\n","epoch : 09 step : 2801 loss = 0.242254 accuracy= 0.812500\n","epoch : 09 step : 2901 loss = 0.341835 accuracy= 0.843750\n","epoch : 09 step : 3001 loss = 0.275647 accuracy= 0.781250\n","epoch : 09 step : 3101 loss = 0.085796 accuracy= 1.000000\n","epoch : 09 step : 3201 loss = 0.167757 accuracy= 0.906250\n","epoch : 09 step : 3301 loss = 0.325056 accuracy= 0.875000\n","epoch : 09 step : 3401 loss = 0.196099 accuracy= 0.937500\n","epoch : 09 step : 3501 loss = 0.095734 accuracy= 0.937500\n","epoch : 09 step : 3601 loss = 0.178724 accuracy= 0.906250\n","epoch : 09 step : 3701 loss = 0.125062 accuracy= 1.000000\n","epoch : 09 step : 3801 loss = 0.134951 accuracy= 0.906250\n","epoch : 09 step : 3901 loss = 0.285093 accuracy= 0.875000\n","epoch : 09 step : 4001 loss = 0.413861 accuracy= 0.843750\n","epoch : 09 step : 4101 loss = 0.333734 accuracy= 0.875000\n","epoch : 09 step : 4201 loss = 0.181109 accuracy= 0.906250\n","epoch : 09 step : 4301 loss = 0.309057 accuracy= 0.875000\n","epoch : 09 step : 4401 loss = 0.201351 accuracy= 0.937500\n","epoch : 09 step : 4501 loss = 0.322760 accuracy= 0.843750\n","epoch : 09 step : 4601 loss = 0.327379 accuracy= 0.843750\n","epoch : 10 step : 0001 loss = 0.120400 accuracy= 0.968750\n","epoch : 10 step : 0101 loss = 0.158239 accuracy= 0.968750\n","epoch : 10 step : 0201 loss = 0.334788 accuracy= 0.875000\n","epoch : 10 step : 0301 loss = 0.299988 accuracy= 0.843750\n","epoch : 10 step : 0401 loss = 0.160294 accuracy= 0.906250\n","epoch : 10 step : 0501 loss = 0.234791 accuracy= 0.843750\n","epoch : 10 step : 0601 loss = 0.224761 accuracy= 0.937500\n","epoch : 10 step : 0701 loss = 0.190881 accuracy= 0.937500\n","epoch : 10 step : 0801 loss = 0.168680 accuracy= 0.906250\n","epoch : 10 step : 0901 loss = 0.121690 accuracy= 0.968750\n","epoch : 10 step : 1001 loss = 0.092155 accuracy= 0.968750\n","epoch : 10 step : 1101 loss = 0.374235 accuracy= 0.781250\n","epoch : 10 step : 1201 loss = 0.243667 accuracy= 0.875000\n","epoch : 10 step : 1301 loss = 0.067157 accuracy= 1.000000\n","epoch : 10 step : 1401 loss = 0.093901 accuracy= 0.968750\n","epoch : 10 step : 1501 loss = 0.186428 accuracy= 0.937500\n","epoch : 10 step : 1601 loss = 0.149758 accuracy= 0.906250\n","epoch : 10 step : 1701 loss = 0.145763 accuracy= 0.937500\n","epoch : 10 step : 1801 loss = 0.251189 accuracy= 0.875000\n","epoch : 10 step : 1901 loss = 0.237246 accuracy= 0.906250\n","epoch : 10 step : 2001 loss = 0.346467 accuracy= 0.843750\n","epoch : 10 step : 2101 loss = 0.131069 accuracy= 0.968750\n","epoch : 10 step : 2201 loss = 0.202332 accuracy= 0.906250\n","epoch : 10 step : 2301 loss = 0.135108 accuracy= 0.968750\n","epoch : 10 step : 2401 loss = 0.369809 accuracy= 0.875000\n","epoch : 10 step : 2501 loss = 0.249232 accuracy= 0.843750\n","epoch : 10 step : 2601 loss = 0.176721 accuracy= 0.937500\n","epoch : 10 step : 2701 loss = 0.214143 accuracy= 0.937500\n","epoch : 10 step : 2801 loss = 0.177794 accuracy= 0.968750\n","epoch : 10 step : 2901 loss = 0.103460 accuracy= 0.968750\n","epoch : 10 step : 3001 loss = 0.281679 accuracy= 0.843750\n","epoch : 10 step : 3101 loss = 0.191264 accuracy= 0.906250\n","epoch : 10 step : 3201 loss = 0.444081 accuracy= 0.843750\n","epoch : 10 step : 3301 loss = 0.266152 accuracy= 0.875000\n","epoch : 10 step : 3401 loss = 0.281497 accuracy= 0.906250\n","epoch : 10 step : 3501 loss = 0.197152 accuracy= 0.906250\n","epoch : 10 step : 3601 loss = 0.283585 accuracy= 0.875000\n","epoch : 10 step : 3701 loss = 0.193032 accuracy= 0.906250\n","epoch : 10 step : 3801 loss = 0.157597 accuracy= 0.937500\n","epoch : 10 step : 3901 loss = 0.136396 accuracy= 0.937500\n","epoch : 10 step : 4001 loss = 0.487386 accuracy= 0.812500\n","epoch : 10 step : 4101 loss = 0.300318 accuracy= 0.937500\n","epoch : 10 step : 4201 loss = 0.168621 accuracy= 0.968750\n","epoch : 10 step : 4301 loss = 0.280239 accuracy= 0.875000\n","epoch : 10 step : 4401 loss = 0.394198 accuracy= 0.875000\n","epoch : 10 step : 4501 loss = 0.096991 accuracy= 0.968750\n","epoch : 10 step : 4601 loss = 0.156353 accuracy= 0.937500\n","epoch : 11 step : 0001 loss = 0.158325 accuracy= 0.937500\n","epoch : 11 step : 0101 loss = 0.116982 accuracy= 0.937500\n","epoch : 11 step : 0201 loss = 0.087055 accuracy= 0.968750\n","epoch : 11 step : 0301 loss = 0.150057 accuracy= 0.937500\n","epoch : 11 step : 0401 loss = 0.102982 accuracy= 0.937500\n","epoch : 11 step : 0501 loss = 0.154308 accuracy= 0.937500\n","epoch : 11 step : 0601 loss = 0.120220 accuracy= 1.000000\n","epoch : 11 step : 0701 loss = 0.121942 accuracy= 0.968750\n","epoch : 11 step : 0801 loss = 0.170169 accuracy= 0.937500\n","epoch : 11 step : 0901 loss = 0.226289 accuracy= 0.937500\n","epoch : 11 step : 1001 loss = 0.071196 accuracy= 0.968750\n","epoch : 11 step : 1101 loss = 0.228778 accuracy= 0.875000\n","epoch : 11 step : 1201 loss = 0.245791 accuracy= 0.843750\n","epoch : 11 step : 1301 loss = 0.061976 accuracy= 1.000000\n","epoch : 11 step : 1401 loss = 0.283115 accuracy= 0.906250\n","epoch : 11 step : 1501 loss = 0.112368 accuracy= 0.968750\n","epoch : 11 step : 1601 loss = 0.192989 accuracy= 0.906250\n","epoch : 11 step : 1701 loss = 0.155116 accuracy= 0.968750\n","epoch : 11 step : 1801 loss = 0.102060 accuracy= 1.000000\n","epoch : 11 step : 1901 loss = 0.226540 accuracy= 0.875000\n","epoch : 11 step : 2001 loss = 0.331496 accuracy= 0.906250\n","epoch : 11 step : 2101 loss = 0.238834 accuracy= 0.937500\n","epoch : 11 step : 2201 loss = 0.153503 accuracy= 0.906250\n","epoch : 11 step : 2301 loss = 0.147090 accuracy= 0.937500\n","epoch : 11 step : 2401 loss = 0.254296 accuracy= 0.906250\n","epoch : 11 step : 2501 loss = 0.196144 accuracy= 0.906250\n","epoch : 11 step : 2601 loss = 0.160820 accuracy= 0.906250\n","epoch : 11 step : 2701 loss = 0.339408 accuracy= 0.843750\n","epoch : 11 step : 2801 loss = 0.289365 accuracy= 0.875000\n","epoch : 11 step : 2901 loss = 0.216601 accuracy= 0.875000\n","epoch : 11 step : 3001 loss = 0.214270 accuracy= 0.875000\n","epoch : 11 step : 3101 loss = 0.236354 accuracy= 0.875000\n","epoch : 11 step : 3201 loss = 0.132721 accuracy= 0.968750\n","epoch : 11 step : 3301 loss = 0.234164 accuracy= 0.843750\n","epoch : 11 step : 3401 loss = 0.094506 accuracy= 1.000000\n","epoch : 11 step : 3501 loss = 0.184506 accuracy= 0.906250\n","epoch : 11 step : 3601 loss = 0.099845 accuracy= 0.968750\n","epoch : 11 step : 3701 loss = 0.215217 accuracy= 0.906250\n","epoch : 11 step : 3801 loss = 0.197780 accuracy= 0.875000\n","epoch : 11 step : 3901 loss = 0.350698 accuracy= 0.843750\n","epoch : 11 step : 4001 loss = 0.144399 accuracy= 0.968750\n","epoch : 11 step : 4101 loss = 0.242034 accuracy= 0.906250\n","epoch : 11 step : 4201 loss = 0.155777 accuracy= 0.937500\n","epoch : 11 step : 4301 loss = 0.160251 accuracy= 0.937500\n","epoch : 11 step : 4401 loss = 0.214831 accuracy= 0.875000\n","epoch : 11 step : 4501 loss = 0.114491 accuracy= 0.937500\n","epoch : 11 step : 4601 loss = 0.112807 accuracy= 0.968750\n","epoch : 12 step : 0001 loss = 0.134928 accuracy= 1.000000\n","epoch : 12 step : 0101 loss = 0.137730 accuracy= 0.968750\n","epoch : 12 step : 0201 loss = 0.096202 accuracy= 1.000000\n","epoch : 12 step : 0301 loss = 0.200199 accuracy= 0.937500\n","epoch : 12 step : 0401 loss = 0.174394 accuracy= 0.875000\n","epoch : 12 step : 0501 loss = 0.259896 accuracy= 0.875000\n","epoch : 12 step : 0601 loss = 0.097715 accuracy= 0.968750\n","epoch : 12 step : 0701 loss = 0.225954 accuracy= 0.906250\n","epoch : 12 step : 0801 loss = 0.284576 accuracy= 0.906250\n","epoch : 12 step : 0901 loss = 0.093102 accuracy= 0.968750\n","epoch : 12 step : 1001 loss = 0.145630 accuracy= 0.906250\n","epoch : 12 step : 1101 loss = 0.147460 accuracy= 0.937500\n","epoch : 12 step : 1201 loss = 0.153813 accuracy= 0.968750\n","epoch : 12 step : 1301 loss = 0.048256 accuracy= 0.968750\n","epoch : 12 step : 1401 loss = 0.190362 accuracy= 0.937500\n","epoch : 12 step : 1501 loss = 0.157343 accuracy= 0.937500\n","epoch : 12 step : 1601 loss = 0.243219 accuracy= 0.937500\n","epoch : 12 step : 1701 loss = 0.112956 accuracy= 0.968750\n","epoch : 12 step : 1801 loss = 0.082804 accuracy= 1.000000\n","epoch : 12 step : 1901 loss = 0.143708 accuracy= 0.968750\n","epoch : 12 step : 2001 loss = 0.209453 accuracy= 0.875000\n","epoch : 12 step : 2101 loss = 0.120325 accuracy= 0.968750\n","epoch : 12 step : 2201 loss = 0.280413 accuracy= 0.875000\n","epoch : 12 step : 2301 loss = 0.372631 accuracy= 0.781250\n","epoch : 12 step : 2401 loss = 0.128014 accuracy= 0.937500\n","epoch : 12 step : 2501 loss = 0.200394 accuracy= 0.937500\n","epoch : 12 step : 2601 loss = 0.112957 accuracy= 0.937500\n","epoch : 12 step : 2701 loss = 0.121128 accuracy= 0.937500\n","epoch : 12 step : 2801 loss = 0.267361 accuracy= 0.843750\n","epoch : 12 step : 2901 loss = 0.120436 accuracy= 1.000000\n","epoch : 12 step : 3001 loss = 0.054737 accuracy= 1.000000\n","epoch : 12 step : 3101 loss = 0.254710 accuracy= 0.875000\n","epoch : 12 step : 3201 loss = 0.106595 accuracy= 1.000000\n","epoch : 12 step : 3301 loss = 0.140138 accuracy= 0.937500\n","epoch : 12 step : 3401 loss = 0.372160 accuracy= 0.875000\n","epoch : 12 step : 3501 loss = 0.182090 accuracy= 0.875000\n","epoch : 12 step : 3601 loss = 0.391887 accuracy= 0.843750\n","epoch : 12 step : 3701 loss = 0.166931 accuracy= 0.937500\n","epoch : 12 step : 3801 loss = 0.160157 accuracy= 0.968750\n","epoch : 12 step : 3901 loss = 0.160063 accuracy= 0.937500\n","epoch : 12 step : 4001 loss = 0.218672 accuracy= 0.937500\n","epoch : 12 step : 4101 loss = 0.356974 accuracy= 0.906250\n","epoch : 12 step : 4201 loss = 0.218903 accuracy= 0.937500\n","epoch : 12 step : 4301 loss = 0.155667 accuracy= 0.937500\n","epoch : 12 step : 4401 loss = 0.418579 accuracy= 0.875000\n","epoch : 12 step : 4501 loss = 0.068544 accuracy= 0.968750\n","epoch : 12 step : 4601 loss = 0.274936 accuracy= 0.906250\n","epoch : 13 step : 0001 loss = 0.125282 accuracy= 0.937500\n","epoch : 13 step : 0101 loss = 0.048391 accuracy= 1.000000\n","epoch : 13 step : 0201 loss = 0.194462 accuracy= 0.937500\n","epoch : 13 step : 0301 loss = 0.171298 accuracy= 0.906250\n","epoch : 13 step : 0401 loss = 0.064603 accuracy= 1.000000\n","epoch : 13 step : 0501 loss = 0.116303 accuracy= 0.968750\n","epoch : 13 step : 0601 loss = 0.268424 accuracy= 0.906250\n","epoch : 13 step : 0701 loss = 0.269626 accuracy= 0.875000\n","epoch : 13 step : 0801 loss = 0.130046 accuracy= 0.968750\n","epoch : 13 step : 0901 loss = 0.124821 accuracy= 0.968750\n","epoch : 13 step : 1001 loss = 0.127775 accuracy= 0.937500\n","epoch : 13 step : 1101 loss = 0.181767 accuracy= 0.937500\n","epoch : 13 step : 1201 loss = 0.104955 accuracy= 0.937500\n","epoch : 13 step : 1301 loss = 0.118387 accuracy= 0.937500\n","epoch : 13 step : 1401 loss = 0.301530 accuracy= 0.875000\n","epoch : 13 step : 1501 loss = 0.105141 accuracy= 0.968750\n","epoch : 13 step : 1601 loss = 0.183799 accuracy= 0.937500\n","epoch : 13 step : 1701 loss = 0.086633 accuracy= 0.968750\n","epoch : 13 step : 1801 loss = 0.100693 accuracy= 0.937500\n","epoch : 13 step : 1901 loss = 0.247428 accuracy= 0.937500\n","epoch : 13 step : 2001 loss = 0.144866 accuracy= 0.937500\n","epoch : 13 step : 2101 loss = 0.294161 accuracy= 0.906250\n","epoch : 13 step : 2201 loss = 0.200438 accuracy= 0.906250\n","epoch : 13 step : 2301 loss = 0.196153 accuracy= 0.875000\n","epoch : 13 step : 2401 loss = 0.189682 accuracy= 0.937500\n","epoch : 13 step : 2501 loss = 0.230354 accuracy= 0.968750\n","epoch : 13 step : 2601 loss = 0.101106 accuracy= 0.937500\n","epoch : 13 step : 2701 loss = 0.278909 accuracy= 0.843750\n","epoch : 13 step : 2801 loss = 0.056092 accuracy= 1.000000\n","epoch : 13 step : 2901 loss = 0.281634 accuracy= 0.812500\n","epoch : 13 step : 3001 loss = 0.191921 accuracy= 0.906250\n","epoch : 13 step : 3101 loss = 0.235174 accuracy= 0.937500\n","epoch : 13 step : 3201 loss = 0.159685 accuracy= 0.968750\n","epoch : 13 step : 3301 loss = 0.200367 accuracy= 0.906250\n","epoch : 13 step : 3401 loss = 0.182908 accuracy= 0.906250\n","epoch : 13 step : 3501 loss = 0.076023 accuracy= 0.968750\n","epoch : 13 step : 3601 loss = 0.131017 accuracy= 0.937500\n","epoch : 13 step : 3701 loss = 0.193181 accuracy= 0.906250\n","epoch : 13 step : 3801 loss = 0.069007 accuracy= 0.968750\n","epoch : 13 step : 3901 loss = 0.102431 accuracy= 0.937500\n","epoch : 13 step : 4001 loss = 0.209443 accuracy= 0.875000\n","epoch : 13 step : 4101 loss = 0.174961 accuracy= 0.968750\n","epoch : 13 step : 4201 loss = 0.254336 accuracy= 0.875000\n","epoch : 13 step : 4301 loss = 0.159758 accuracy= 0.968750\n","epoch : 13 step : 4401 loss = 0.109528 accuracy= 0.937500\n","epoch : 13 step : 4501 loss = 0.175594 accuracy= 0.937500\n","epoch : 13 step : 4601 loss = 0.305981 accuracy= 0.812500\n","Save Complete\n","save_path :  /content/gdrive/My Drive/Colab Notebooks/GraduationProject/Bi_LSTM/BiLSTM_model_Epoch_13.ckpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"phvcj_1vVA3c","colab_type":"code","outputId":"8a6d1ee6-c957-4981-c739-bef6442eff69","executionInfo":{"status":"ok","timestamp":1586710720745,"user_tz":-540,"elapsed":906,"user":{"displayName":"박회재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLNtW3cEP6ijcjRJLZBANmZYUyTrHXcKJWEGtNDA=s64","userId":"17044052781042873598"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Whole learning time (s)\n","\n","14409/60"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["240.15"]},"metadata":{"tags":[]},"execution_count":24}]}]}